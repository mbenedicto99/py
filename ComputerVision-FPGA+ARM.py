"""
# Finalidade  : MLP com 4 entradas, 3 neurÃ´nios ocultos e 2 saÃ­das
# Autor       : Marcos de Benedicto (IA)
# Data        : 22/03/2025
# Input       : 
# Output      :
Entrada X: [ 1.   0.5 -1.5  2. ]
AtivaÃ§Ã£o camada oculta: [1.85  0.35  0.565]
SaÃ­da Y: [0.93  0.363]

Simularemos o produto ğ‘Œ=ğ‘“(ğ‘Šâ‹…ğ‘‹)
Y=f(Wâ‹…X), como no bloco (b) da imagem.
A funÃ§Ã£o de ativaÃ§Ã£o serÃ¡ ReLU (representando um processamento simples no FPGA).
Os pesos ğ‘ŠW sÃ£o fixos, como os valores de condutÃ¢ncia dos memristores.
O script mostra como aplicar uma entrada vetorial ğ‘‹
X a uma rede neural com uma camada oculta."""

import numpy as np

# --- FunÃ§Ãµes auxiliares ---
def relu(x):
    return np.maximum(0, x)

# --- Entradas (vetor simulado de uma imagem ou sensor) ---
X = np.array([1.0, 0.5, -1.5, 2.0])  # entrada de 4 elementos (pode vir do DAC)

# --- Pesos simulando condutÃ¢ncias de memristores ---
# Pesos da camada de entrada para camada oculta (4 entradas x 3 neurÃ´nios)
W1 = np.array([
    [0.8, -0.5, 0.3],
    [0.2,  0.7, 0.1],
    [-0.3, 0.9, -0.4],
    [0.5, 0.1, -0.6]
])

# Bias da camada oculta
b1 = np.array([0.1, -0.2, 0.05])

# Pesos da camada oculta para camada de saÃ­da (3 x 2)
W2 = np.array([
    [0.6, -0.1],
    [-0.3, 0.8],
    [0.4, 0.2]
])

# Bias da saÃ­da
b2 = np.array([0.0, 0.1])

# --- InferÃªncia (simulaÃ§Ã£o do hardware) ---
# Etapa 1: produto matriz-vetor com W1
Z1 = np.dot(X, W1) + b1
A1 = relu(Z1)  # simula ativaÃ§Ã£o ReLU no FPGA/ARM

# Etapa 2: produto matriz-vetor com W2
Z2 = np.dot(A1, W2) + b2
Y = relu(Z2)  # saÃ­da final da rede

# --- Resultados ---
print("Entrada X:", X)
print("AtivaÃ§Ã£o camada oculta:", A1)
print("SaÃ­da Y:", Y)
